{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div style='background-image: url(\"../../share/images/header.svg\") ; padding: 0px ; background-size: cover ; border-radius: 5px ; height: 250px'>\n",
    "    <div style=\"float: right ; margin: 50px ; padding: 20px ; background: rgba(255 , 255 , 255 , 0.7) ; width: 50% ; height: 150px\">\n",
    "        <div style=\"position: relative ; top: 50% ; transform: translatey(-50%)\">\n",
    "            <div style=\"font-size: xx-large ; font-weight: 900 ; color: rgba(0 , 0 , 0 , 0.8) ; line-height: 100%\">Computational Seismology</div>\n",
    "            <div style=\"font-size: large ; padding-top: 20px ; color: rgba(0 , 0 , 0 , 0.5)\">Parallel Computing - An introduction to MPI4Py</div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seismo-Live: http://seismo-live.org\n",
    "\n",
    "##### Authors:\n",
    "* David Vargas ([@dvargas](https://github.com/davofis))\n",
    "* Heiner Igel ([@heinerigel](https://github.com/heinerigel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers the following aspects:\n",
    "\n",
    "* Introduction to parallel computing\n",
    "* Serial vr parallel computing \n",
    "* Flynn's Classical Taxonomy\n",
    "* General concepts and terminology\n",
    "* Amdahl's Law\n",
    "* Introduction to MPI4Py\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Parallel Computing\n",
    "\n",
    "In parallel computing many calculations are executed simultaneously. The real world is massively parallel since many independent and complex processes occur at the same time. This notebook is intended as an introduction to some of the most basic concepts in parallel programming with MPI4Py, a module providing standard functions to perform tasks under the standard of the message passing interface MPI.  \n",
    "\n",
    "### 1.1 Serial vr parallel computing\n",
    "A parallel computer can be understood as a set of processors working together to perform an specific task. In contrary, serial computers execute one instruction at the time in a single processor till the task is completed (See figures below). Some of the most relevant characteristics are listed: \n",
    "\n",
    "<p style=\"width:30%;float:right;padding-left:10px;padding-right:20px\">\n",
    "<img src=images/Parallel_Computing.png>\n",
    "\n",
    "<p style=\"width:30%;float:right;padding-left:10px;padding-right:20px\">\n",
    "<img src=images/Serial_Computing.png>\n",
    "\n",
    "Serial computing:\n",
    "* A problem is broken into a discrete series of instructions\n",
    "* Instructions are executed sequentially one after another\n",
    "* Executed on a single processor\n",
    "* Only one instruction may execute at any moment in time \n",
    "\n",
    "Parallel computing:\n",
    "* A problem is broken into discrete parts that can be solved concurrently\n",
    "* Each part is further broken down to a series of instructions\n",
    "* Instructions from each part execute simultaneously on different processors\n",
    "* An overall control/coordination mechanism is employed\n",
    "\n",
    "\n",
    "### 1.2 Flynn's Classical Taxonomy\n",
    "<p style=\"width:40%;float:right;padding-left:40px;padding-right:30px\">\n",
    "<img src=images/flynn.png>\n",
    "\n",
    "<div style=\"text-align: justify\">\n",
    "Flynn's taxonomy is a computer architecture classification, it has been used since 1966 and is based on the number of concurrent instruction (single or multiple) and data streams  (single or multiple) available in the architecture. The four categories in Flynn's taxonomy are the following:   \n",
    "</div>\n",
    "* Single Instruction stream Single Data stream (SISD)\n",
    "* Single Instruction stream, Multiple Data streams (SIMD)\n",
    "* Multiple instruction streams, single data stream (MISD)\n",
    "* Multiple instruction streams, multiple data streams (MIMD)\n",
    "\n",
    "### 1.3 General Concepts and Terminology\n",
    "* **Node:** A single computer. Nodes are networked together to comprise a supercomputer.\n",
    "* **CPU / Socket / Processor / Core:** CPUs with multiple cores are sometimes called \"sockets\" \n",
    "* **Task:** A Discrete section of computational work\n",
    "* **Pipelining:** Breaking a task into steps that are performed by different processor units\n",
    "* **Shared Memory:** Computer architecture where all processors have direct access to common physical memory\n",
    "* **Distributed Memory:** Computer architecture where access to physical memory is not common\n",
    "* **Embarrassingly Parallel:** Solving many similar, but independent tasks simultaneously\n",
    "* **Scalability:** Refers to an increase in speedup with the addition of more resources\n",
    "\n",
    "### 1.4 Amdahl's Law\n",
    "<div style=\"text-align: justify\">\n",
    "The scalability or potential program speedup is defined by the fraction of code (P) that can be parallelized. \n",
    "Amdahl's Law determine the speedup when using parallel processors on a program in contrast with the serial case. We define speedup as the time it takes a program to execute in serial (with one processor) divided by the time it takes to execute in parallel (with many processors), i.e.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\\begin{equation}\n",
    "Speedup = \\frac{t(1)}{t(n)} = \\frac{1}{\\frac{p}{n} + s}\n",
    "\\end{equation}\n",
    "\n",
    "</div>\n",
    "* $p$ = Fraction of code can be parallelized\n",
    "* $n$ = Number of processors\n",
    "* $s$ = Serial fraction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to MPI4Py\n",
    "Python code is one of the most famous languages used by scientific community. Indeed, sharing code to reproduce simulations has gain relevance in recent years [see this nature article](http://www.nature.com/news/interactive-notebooks-sharing-the-code-1.16261). In this sense, parallel computing with python is essential to execute algorithms as efficient as possible. In order to glue MPI with Python, mpi4py emerge as a way to do so. mpi4py provides python bindings to most of the functionality of the MPI standard of the message passing interface. The official web site for mpi4py is [mpi4py.scipy.org](http://mpi4py.scipy.org/), additionally, you can find a [user manual](http://mpi4py.scipy.org/docs/usrman/index.html) and a [API reference](http://mpi4py.scipy.org/docs/apiref/index.html) for a more comprehensive description. \n",
    "\n",
    "### Getting started\n",
    "This notebook only gives a short description of the principal elements contained in the mpi4py package. Before you start, make sure you have launched a new Ipython cluster with the desired number of engines. Having done that, run the Ipython cluster setup cell as well as the imports cell, this will allow you to use MPI4Py into the jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ipython cluster setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import all necessary libraries, this is a configuration step for the exercise.\n",
    "# Please run it before the simulation code!\n",
    "from ipyparallel import Client\n",
    "cluster = Client(profile='mpi')\n",
    "cluster.block = True  # use synchronous computations\n",
    "dview = cluster[:]\n",
    "dview.activate()      # enable magics\n",
    "\n",
    "cluster.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Libraries are imported on all workers. This is a configuration step for the exercise. Please run it before the simulation code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Show the plots in the Notebook.\n",
    "#%pylab inline\n",
    "#==================================\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "name = MPI.Get_processor_name()\n",
    "#=================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hello World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "print(\"Hello, World! I am process %d of %d on %s.\\n\" % (rank, size, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Operations on different processors I "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "# Variables defined on each processor\n",
    "a = 2\n",
    "b = 3\n",
    "\n",
    "if rank == 0:\n",
    "    print('a + b = %d' %(a + b))\n",
    "if rank == 1:\n",
    "    print ('a * b = %d' %(a * b))\n",
    "if rank == 2:\n",
    "    print ('a / b = %d' %(a / b))\n",
    "if rank == 3:\n",
    "    print ('max(a,b) = %d' %max(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Operations on different processors II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     14,
     21,
     54
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "#------------------------------------------------------------------\n",
    "# Processor 0\n",
    "#------------------------------------------------------------------\n",
    "if rank == 0:\n",
    "    fig = plt.figure(figsize=(6,5))\n",
    "    t = np.arange(0.0, 6.0, 0.01)\n",
    "    y1 = np.exp(-t) * np.cos(2*np.pi*t)\n",
    "    y2 = np.exp(-t)\n",
    "    y3 = -np.exp(-t)\n",
    "    plt.plot(t, y2,'b--', t, y3,'b--',t, y1,'g', lw=2)\n",
    "#------------------------------------------------------------------\n",
    "# Processor 1\n",
    "#------------------------------------------------------------------\n",
    "if rank == 1:\n",
    "    fig = plt.figure(figsize=(6,5))\n",
    "    plt.plot(np.random.rand(100),np.random.rand(100), 'r')\n",
    "    \n",
    "#------------------------------------------------------------------\n",
    "# Processor 2\n",
    "#------------------------------------------------------------------    \n",
    "if rank == 2:\n",
    "    def lorenz(x, y, z, s=10, r=28, b=2.667):\n",
    "        x_dot = s*(y - x)\n",
    "        y_dot = r*x - y - x*z\n",
    "        z_dot = x*y - b*z\n",
    "        return x_dot, y_dot, z_dot\n",
    "\n",
    "    dt = 0.01\n",
    "    stepCnt = 10000\n",
    "\n",
    "    # Need one more for the initial values\n",
    "    xs = np.empty((stepCnt + 1,))\n",
    "    ys = np.empty((stepCnt + 1,))\n",
    "    zs = np.empty((stepCnt + 1,))\n",
    "\n",
    "    # Setting initial values\n",
    "    xs[0], ys[0], zs[0] = (0., 1., 1.05)\n",
    "\n",
    "    # Stepping through \"time\".\n",
    "    for i in range(stepCnt):\n",
    "        # Derivatives of the X, Y, Z state\n",
    "        x_dot, y_dot, z_dot = lorenz(xs[i], ys[i], zs[i])\n",
    "        xs[i + 1] = xs[i] + (x_dot * dt)\n",
    "        ys[i + 1] = ys[i] + (y_dot * dt)\n",
    "        zs[i + 1] = zs[i] + (z_dot * dt)\n",
    "\n",
    "    fig = plt.figure(figsize=(6,5))\n",
    "    ax = fig.gca(projection='3d')\n",
    "\n",
    "    ax.plot(xs, ys, zs, lw=0.2)\n",
    "#------------------------------------------------------------------\n",
    "# Processor 3\n",
    "#------------------------------------------------------------------\n",
    "if rank == 3:\n",
    "    # Fixing random state for reproducibility\n",
    "    np.random.seed(19680801)\n",
    "\n",
    "    mu, sigma = 100, 15\n",
    "    x = mu + sigma * np.random.randn(10000)\n",
    "\n",
    "    # the histogram of the data\n",
    "    fig = plt.figure(figsize=(6,5))\n",
    "    n, bins, patches = plt.hist(x, 50, normed=1, facecolor='g', alpha=0.75)\n",
    "    \n",
    "    plt.xlabel('Smarts')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title('Histogram of IQ')\n",
    "    plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "    plt.axis([40, 160, 0, 0.03])\n",
    "    plt.grid(True)\n",
    "    \n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Send, recv\n",
    "<div style=\"text-align: justify\">  \n",
    "\n",
    "<p style=\"width:40%;float:right;padding-left:40px;padding-right:60px\">\n",
    "<img src=images/point_point.png>\n",
    "<span style=\"font-size:smaller\">\n",
    "</span>\n",
    "</p>\n",
    "\n",
    "Send a message from one process to another one is known as point to point communication. In the message passing programming model it is done in a two step process, one task sends data while another receives data. The sending task must specify the data to be sent, their destination and an identifier for the message. The receiving task has to specify the location for storing the received data, their source and the identifier of the message to be received. \n",
    "\n",
    "</div>\n",
    "\n",
    "Method for sending: \n",
    "```\n",
    "comm.send(buf, dest=0, tag=0)\n",
    "\n",
    "```\n",
    "Method for receiving: \n",
    "```\n",
    "comm.recv(obj=None, source=0, tag=0, status=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "data = None\n",
    "if rank == 0:\n",
    "    data = (1, 'a', 'z', 3.14)\n",
    "    comm.send(data, dest=1, tag=11)\n",
    "    print('On rank', rank, ':  data = ', data)\n",
    "elif rank == 1:\n",
    "    print('On rank', rank, 'before recv:  data = ', data)\n",
    "    data = comm.recv(source=0, tag=11)\n",
    "    print('On rank', rank, 'after  recv:  data = ', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4. Broadcast\n",
    "<div style=\"text-align: justify\">  \n",
    "\n",
    "<p style=\"width:40%;float:right;padding-left:40px;padding-right:60px\">\n",
    "<img src=images/Bcast.png>\n",
    "<span style=\"font-size:smaller\">\n",
    "</span>\n",
    "</p>\n",
    "\n",
    "Multiple processes exchanging data with the same communicator is known as collective communication. Typical operations include Broadcast, Scatter, Gather, Reduction. We start by describing the syntax of Broadcast, it allows a user to send data that is generated by the master to all workers at once. Basically we broadcast data from master/root engine to all the other (workers) engines. The syntax of the Bcast() method is:   \n",
    "</div>\n",
    "\n",
    "```\n",
    "comm.Bcast(buf, root=0)\n",
    "buf = comm.bcast(obj=None, root=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "print(\"-\"*20)\n",
    "print('On Rank:  ', rank)\n",
    "print(\"-\"*20)\n",
    "# Prepare a vector of N=5 elements to be broadcasted...\n",
    "N = 5\n",
    "if rank == 0:\n",
    "    A = np.arange(N, dtype=np.float64)    # rank 0 has proper data    \n",
    "    print(\"Before broadcast A = %s\" % A)\n",
    "else:\n",
    "    A = np.zeros(N, dtype=np.float64)     # all other just an empty array\n",
    "    print(\"Before broadcast A = %s\" % A)\n",
    "\n",
    "# Broadcast A from rank 0 to everybody\n",
    "comm.Bcast( [A, MPI.DOUBLE] )\n",
    "\n",
    "# Everybody should now have the same...\n",
    "for r in range(size):\n",
    "    if rank == r:\n",
    "        print(\"After  broadcast A = %s\" % A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Scatter\n",
    "<div style=\"text-align: justify\">  \n",
    "\n",
    "<p style=\"width:40%;float:right;padding-left:40px;padding-right:60px\">\n",
    "<img src=images/Scatter.png>\n",
    "<span style=\"font-size:smaller\">\n",
    "</span>\n",
    "</p>\n",
    "Scatter is the process of breaking up data and distributing each part to all processors. Whereas broadcast distributes the same object from the root engine to all others, scattering sends specific part of the data from root to all processors. The syntax of the scatter() method is  \n",
    "</div>\n",
    "\n",
    "```\n",
    "comm.Scatter(sendbuf, recvbuf, root=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "print(\"-\"*20)\n",
    "print('On Rank:  ', rank)\n",
    "print(\"-\"*20)\n",
    "\n",
    "n = 2\n",
    "A = np.zeros(n, dtype=np.float64)\n",
    "\n",
    "if rank == 3:\n",
    "    data = np.arange(n*size, dtype=np.float64) + 1\n",
    "    print('Data to be scattered: data = %s' % data)\n",
    "    print('Before scatter: A = %s' % A)\n",
    "else:\n",
    "    data = np.zeros(n*size, dtype=np.float64)\n",
    "    print('Before scatter: A = %s' % A)\n",
    "\n",
    "comm.Scatter( [data, MPI.DOUBLE], [A, MPI.DOUBLE] )\n",
    "\n",
    "print(\"After  scatter: A = %s\" % A)\n",
    "print('') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Gather\n",
    "<div style=\"text-align: justify\">  \n",
    "\n",
    "<p style=\"width:40%;float:right;padding-left:40px;padding-right:60px\">\n",
    "<img src=images/Gather.png>\n",
    "<span style=\"font-size:smaller\">\n",
    "</span>\n",
    "</p>\n",
    "\n",
    "Gather is the opposite of scatter. It is used to collect data from various cores and store it as one in a single core. The syntax of the Gather() methods is  \n",
    "</div>\n",
    "\n",
    "```\n",
    "comm.Gather(sendbuf, recvbuf, root=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "print(\"-\"*20)\n",
    "print('On Rank:  ', rank)\n",
    "print(\"-\"*20)\n",
    "\n",
    "n = 2\n",
    "A = np.zeros(n, dtype=np.float64)\n",
    "\n",
    "data = np.arange(n*size, dtype=np.float64) + 1\n",
    "comm.Scatter( [data, MPI.DOUBLE], [A, MPI.DOUBLE], root=0 )\n",
    "data = np.zeros(n*size, dtype=np.float64) \n",
    "\n",
    "print(\"Before Gather: A = %s\" % A)\n",
    "comm.Gather( [A, MPI.DOUBLE], [data, MPI.DOUBLE], root=3 )\n",
    "print(\"After  Gather: data = %s\" % data)\n",
    "print('') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Allgather\n",
    "<div style=\"text-align: justify\">  \n",
    "\n",
    "<p style=\"width:35%;float:right;padding-left:40px;padding-right:60px\">\n",
    "<img src=images/Allgather.png>\n",
    "<span style=\"font-size:smaller\">\n",
    "</span>\n",
    "</p>\n",
    "The gather operations collects data from all tasks and delivers this collection to the root task. In contrast, allgather delivers this collection of data to all processors. The syntax of the Allgather() methods is \n",
    "</div>\n",
    "```\n",
    "comm.Allgather(sendbuf, recvbuf)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "print(\"-\"*20)\n",
    "print('On Rank:  ', rank)\n",
    "print(\"-\"*20)\n",
    "\n",
    "n = 2\n",
    "A = np.zeros(n, dtype=np.float64)\n",
    "\n",
    "data = np.arange(n*size, dtype=np.float64) + 1\n",
    "comm.Scatter( [data, MPI.DOUBLE], [A, MPI.DOUBLE], root=0 )\n",
    "data = np.zeros(n*size, dtype=np.float64) \n",
    "\n",
    "print(\"Before Allgather: A = %s\" % A)\n",
    "comm.Allgather( [A, MPI.DOUBLE], [data, MPI.DOUBLE] )\n",
    "print(\"After  Allgather: data = %s\" % data)\n",
    "print('')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Reduce\n",
    "<div style=\"text-align: justify\">  \n",
    "\n",
    "<p style=\"width:40%;float:right;padding-left:40px;padding-right:60px\">\n",
    "<img src=images/Reduce.png>\n",
    "<span style=\"font-size:smaller\">\n",
    "</span>\n",
    "</p>\n",
    "Data reduction involves reducing a set of numbers into a smaller set of numbers via a function. reduction methods of MPI combine data from all processors with an specific operation and delivers the result to an specific engine (root). mpi4py provides the following operations for reduction:  \n",
    "</div>\n",
    "\n",
    "```\n",
    "MPI.MIN        minimum\n",
    "MPI.MAX        maximum\n",
    "MPI.SUM        sum\n",
    "MPI.PROD       product\n",
    "MPI.LAND       logical and\n",
    "MPI.BAND       bitwise and\n",
    "MPI.LOR        logical or\n",
    "MPI.BOR        bitwise or\n",
    "MPI.LXOR       logical xor\n",
    "MPI.BXOR       bitwise xor\n",
    "MPI.MAXLOC     max value and location\n",
    "MPI.MINLOC     min value and location\n",
    "```\n",
    "\n",
    "The syntax of the Reduce() method is:\n",
    "```\n",
    "comm.Reduce(sendbuf, recvbuf, op=MPI.SUM, root=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "# Array defined on each processor\n",
    "A = np.zeros(size)\n",
    "for i in range(comm.rank, len(A), comm.size):\n",
    "    # set data in each array \n",
    "    A[i] = i + 1\n",
    "print(\"On rank %d, A = %s\" % (rank, A))\n",
    "\n",
    "if rank == 3:\n",
    "    Reduce = np.zeros(size) # only processor 0 will get the data\n",
    "else:\n",
    "    Reduce = None\n",
    "    \n",
    "comm.Reduce([A, MPI.DOUBLE], [Reduce, MPI.DOUBLE], op = MPI.SUM, root = 0)\n",
    "if rank == 3: print(\"On rank %d, Reduce = %s\" % (rank, Reduce))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Allreduce\n",
    "<div style=\"text-align: justify\">  \n",
    "\n",
    "<p style=\"width:40%;float:right;padding-left:40px;padding-right:60px\">\n",
    "<img src=images/Allreduce.png>\n",
    "<span style=\"font-size:smaller\">\n",
    "</span>\n",
    "</p>\n",
    "Allreduce operates in a similar way compared with Allgather, except that an operation on data is perform and delivered to all processors.\n",
    "   \n",
    "</div>\n",
    "\n",
    "The syntax of the Allreduce() method is:\n",
    "```\n",
    "comm.Allreduce(sendbuf, recvbuf, op=MPI.SUM)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px \n",
    "A = np.zeros(size)\n",
    "for i in range(comm.rank, len(A), comm.size):\n",
    "    # set data in each array \n",
    "    A[i] = i + 1\n",
    "print(\"On rank %d, A = %s\" % (rank, A))\n",
    "\n",
    "total = np.zeros(size)\n",
    "    \n",
    "comm.Allreduce([A, MPI.DOUBLE], [total, MPI.DOUBLE], op = MPI.SUM)\n",
    "\n",
    "print(\"On rank %d, Allreduce = %s\" % (rank, total))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
