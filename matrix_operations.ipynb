{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-image: url(\"../../share/images/header.svg\") ; padding: 0px ; background-size: cover ; border-radius: 5px ; height: 250px'>\n",
    "    <div style=\"float: right ; margin: 50px ; padding: 20px ; background: rgba(255 , 255 , 255 , 0.7) ; width: 50% ; height: 150px\">\n",
    "        <div style=\"position: relative ; top: 50% ; transform: translatey(-50%)\">\n",
    "            <div style=\"font-size: xx-large ; font-weight: 900 ; color: rgba(0 , 0 , 0 , 0.8) ; line-height: 100%\">Computational Seismology</div>\n",
    "            <div style=\"font-size: large ; padding-top: 20px ; color: rgba(0 , 0 , 0 , 0.5)\">Parallel Computing - Matrix Operations with MPI4Py</div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seismo-Live: http://seismo-live.org\n",
    "\n",
    "##### Authors:\n",
    "* David Vargas ([@dvargas](https://github.com/davofis))\n",
    "* Heiner Igel ([@heinerigel](https://github.com/heinerigel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents a series of examples applied to matrix operations with mpi4py:\n",
    "\n",
    "* Parallel dot product\n",
    "* Parallel Matrix-vector product \n",
    "* Parallel Matrix-Matrix product\n",
    "* Matrix algebra\n",
    "* Trapezoidal Rule\n",
    "* Derivatives\n",
    "---\n",
    "\n",
    "### Getting started\n",
    "Before you start, make sure you have launched a new Ipython cluster with the desired number of engines. Having done that, run the Ipython cluster setup cell as well as the imports cell, this will allow you to use MPI4Py into the jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ipython cluster setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import all necessary libraries, this is a configuration step for the exercise.\n",
    "# Please run it before the simulation code!\n",
    "from ipyparallel import Client\n",
    "cluster = Client(profile='mpi')\n",
    "cluster.block = True  # use synchronous computations\n",
    "dview = cluster[:]\n",
    "dview.activate()      # enable magics\n",
    "\n",
    "cluster.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Libraries are imported on all workers. This is a configuration step for the exercise. Please run it before the simulation code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "from mpi4py import MPI\n",
    "from mpi4py.MPI import ANY_SOURCE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Show the plots in the Notebook.\n",
    "#%pylab inline\n",
    "#==================================\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "name = MPI.Get_processor_name()\n",
    "#=================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parallel dot product\n",
    "\n",
    "<div style=\"text-align: justify\">  \n",
    "\n",
    "<p style=\"width:40%;float:right;padding-left:60px;padding-right:40px\">\n",
    "<img src=images/dot_product.png>\n",
    "<span style=\"font-size:smaller\">\n",
    "</span>\n",
    "</p>\n",
    "\n",
    "Given the low data interdependency, the inner product of vectors can be implemented in parallel. The main idea is to split each vector into blocks and distribute them across all processors (see figure on the right). In this sense, domain partitioning allow individual engines to perform local operations. This operation with $N_0, N_1, N_2, \\cdots, N_i$ engines can be written as:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$\n",
    "\\langle x, y\\rangle = \\sum_{i=1}^{N_0} x_iy_i + \\sum_{i=N_0+1}^{N_1} x_iy_i + \\cdots + \\sum_{i=N_{p-1}+1}^{N_p} x_iy_i\\\\\n",
    "$\n",
    "</div>\n",
    "\n",
    "\n",
    "A pseudo code for parallel implementation reads: \n",
    "```\n",
    "1 Initialize vectors\n",
    "2 test vector length match\n",
    "3 Split vectors on engines, domain partitioning\n",
    "4 Compute local dot product\n",
    "5 sum the results of each processor\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px \n",
    "n = 8    #length of vectors\n",
    "\n",
    "if comm.rank == 0: \n",
    "    x = np.random.rand(n)\n",
    "    y = np.random.rand(n)\n",
    "else:\n",
    "    x = None\n",
    "    y = None\n",
    "\n",
    "#initialize as numpy arrays\n",
    "dot = np.array([0.])\n",
    "n_proc = int(n/size)\n",
    "\n",
    "if rank == 0:\n",
    "    #test vector length match\n",
    "    if (x.size != y.size):\n",
    "        print('vector length mismatch')\n",
    "        comm.Abort()   \n",
    "    #Vector sizes must be evenly divided by the number of processors\n",
    "    if(n % size != 0):\n",
    "        print('the number of processors must evenly divide n.')\n",
    "        comm.Abort()\n",
    "    \n",
    "#initialize as numpy arrays\n",
    "x_proc = np.zeros(n_proc)\n",
    "y_proc = np.zeros(n_proc)\n",
    "\n",
    "#divide vectors up\n",
    "comm.Scatter( [x, MPI.DOUBLE], [x_proc, MPI.DOUBLE] )\n",
    "comm.Scatter( [y, MPI.DOUBLE], [y_proc, MPI.DOUBLE] )\n",
    "\n",
    "#local computation of dot product\n",
    "dot_proc = np.array([x_proc @ y_proc])\n",
    "\n",
    "#sum the results of each\n",
    "comm.Reduce([dot_proc, MPI.DOUBLE], [dot, MPI.DOUBLE], op = MPI.SUM, root = 0)\n",
    "\n",
    "if (rank == 0):\n",
    "    print('Parallel dot product :', dot[0])\n",
    "    print('Serial dot product   :', x @ y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parallel Matrix-vector product\n",
    "\n",
    "<div style=\"text-align: justify\">  \n",
    "\n",
    "<p style=\"width:40%;float:right;padding-left:60px;padding-right:40px\">\n",
    "<img src=images/Matrix_product.png>\n",
    "<span style=\"font-size:smaller\">\n",
    "</span>\n",
    "</p>\n",
    "\n",
    "Matrix multiplication is a central operation in many numerical algorithms, developing efficient algorithms to perform this task is a key point in numerical analysis. Given the matrix-vector product\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\\begin{equation}\n",
    "c_i =  \\sum_{j=1}^{N} A_{ij}x_{j}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "We use domain partitioning on matrix A to perform local operations on all available engines. The figure on the right shows an example on how this is done.   \n",
    "</div>\n",
    "\n",
    "A pseudo code for parallel implementation reads: \n",
    "```\n",
    "1 Initialize data\n",
    "2 Test matrix-vector length match\n",
    "3 Split matrix on engines, domain partitioning\n",
    "4 Compute local product\n",
    "5 Collect the results from all processors\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "def mat_vec(comm, A, x):\n",
    "    size = comm.Get_size()\n",
    "    n_proc = int(A.shape[0]/size)\n",
    "    if (A.shape[1] != x.size):\n",
    "        print('vector-matrix size mismatch')\n",
    "        comm.Abort()  \n",
    "    if(x.size % size != 0):\n",
    "        print('the number of processors must evenly divide n.')\n",
    "        comm.Abort()\n",
    "    # Initialize local matrices\n",
    "    A_proc = np.zeros((n_proc, A.shape[1]))  \n",
    "    # Divide and distribute matrice up\n",
    "    comm.Scatter( [A, MPI.DOUBLE], [A_proc, MPI.DOUBLE] )\n",
    "    # Broadcast vector x\n",
    "    comm.Bcast( [x, MPI.DOUBLE] )\n",
    "    #Return local computation of dot product\n",
    "    y_proc = A_proc @ x\n",
    "    return y_proc\n",
    "\n",
    "m = 8\n",
    "n = 4 \n",
    "# Define arrays on all ranks\n",
    "A = np.random.rand(m, n)\n",
    "x = np.arange(n)\n",
    "y = np.zeros(m)\n",
    "\n",
    "# Parallel matrix-vector product \n",
    "y_proc = mat_vec(comm, A, x)\n",
    "# Gather local vectors  \n",
    "comm.Gather( [y_proc, MPI.DOUBLE], [y, MPI.DOUBLE], root=0 )\n",
    "# Display result    \n",
    "if (rank == 0):\n",
    "    print('Parallel matrix product :', y)\n",
    "    print('Serial matrix product   :', A@x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Parallel Matrix-Matrix product\n",
    "<div style=\"text-align: justify\">  \n",
    "\n",
    "<p style=\"width:40%;float:right;padding-left:60px;padding-right:40px\">\n",
    "<img src=images/matrix_matrix.png>\n",
    "<span style=\"font-size:smaller\">\n",
    "</span>\n",
    "</p>\n",
    "\n",
    "Matrix multiplication in computational problems is applied in many fields including scientific computing and pattern recognition. Many different algorithms have been designed for multiplying matrices on different types of hardware, including parallel and distributed systems. The matrix product given by the equation \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\\begin{equation}\n",
    "c_ij =  \\sum_{k=1}^{N} A_{ik}x_{kj}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "can be computed after domain decomposition as it was the case en the previous examples. The figure on the right illustrates a parallel implementation for matrix product\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "def mat_vec(comm, A, B):\n",
    "    size = comm.Get_size()\n",
    "    n_proc = int(A.shape[0]/size)\n",
    "    if (A.shape[1] != B.shape[0]):\n",
    "        print('vector-matrix size mismatch')\n",
    "        comm.Abort()  \n",
    "    if(B.shape[0] % size != 0):\n",
    "        print('the number of processors must evenly divide n.')\n",
    "        comm.Abort()\n",
    "    # Initialize local matrices\n",
    "    A_proc = np.zeros((n_proc, A.shape[1]))  \n",
    "    # Divide and distribute matrice up\n",
    "    comm.Scatter( [A, MPI.DOUBLE], [A_proc, MPI.DOUBLE] )\n",
    "    # Broadcast matrix B\n",
    "    comm.Bcast( [B, MPI.DOUBLE] )\n",
    "    #Return local computation of dot product\n",
    "    C_proc = A_proc @ B\n",
    "    return C_proc\n",
    "\n",
    "m = 8\n",
    "n = 4 \n",
    "p = 3\n",
    "# Define arrays on all ranks\n",
    "A = np.random.rand(m, n)\n",
    "B = np.random.rand(n, p)\n",
    "C = np.zeros((m, p))\n",
    "\n",
    "# Parallel matrix-vector product \n",
    "C_proc = mat_vec(comm, A, B)\n",
    "\n",
    "# Gather local vectors  \n",
    "comm.Gather( [C_proc, MPI.DOUBLE], [C, MPI.DOUBLE], root=0 )\n",
    "\n",
    "# Display result    \n",
    "if (rank == 0):\n",
    "    print('Parallel matrix product :', C)\n",
    "    print('Serial matrix product   :', A @ B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Matrix algebra\n",
    "Domain decomposition can be used to perform aditional numerical operations, the next cell evaluate one of this cases. Given two matrices, $X,Y$, compute $Z = 5X^2 + 2Y^2$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "n = 4; m = 8\n",
    "\n",
    "# Initialize local matrices on all processors\n",
    "n_proc = int(m/size)\n",
    "x_proc = np.zeros((n_proc, n)); y_proc=x_proc\n",
    "\n",
    "if rank == 0:\n",
    "    # Define arrays on rank 0\n",
    "    x = np.ones((m, n)); y=x; z=x\n",
    "\n",
    "    # Check partitioning condition \n",
    "    if(x.shape[0] % size != 0):\n",
    "        print('the number of processors must evenly divide.')\n",
    "        comm.Abort()\n",
    "else:\n",
    "    x = None; y=x; z=x\n",
    "    \n",
    "# Divide and distribute matrice on all ranks\n",
    "comm.Scatter( [x, MPI.DOUBLE], [x_proc, MPI.DOUBLE] )\n",
    "comm.Scatter( [y, MPI.DOUBLE], [y_proc, MPI.DOUBLE] )\n",
    "\n",
    "# Evaluate on each rank\n",
    "z_proc = 5*x_proc**2 + 2*y_proc**2 \n",
    "\n",
    "# Gather local vectors  \n",
    "comm.Gather( [z_proc, MPI.DOUBLE], [z, MPI.DOUBLE], root=0 )\n",
    "\n",
    "# Display solution\n",
    "if rank == 0: \n",
    "    print('On rank %d: ' % rank )\n",
    "    print('z = %s' % z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Trapezoidal Rule\n",
    "The next cell implements a parallel version of the classical trapezoidal rule for numerical integration, this example is originally implemented in [A Python Introduction to Parallel Programming with MPI](http://materials.jeremybejarano.com/MPIwithPython/pointToPoint.html#). It is intended to illustrate how local Point-To-Point communication is used. As it is pointed out in this tutorial, a range to be integrated is divided into many vertical slivers, and each sliver is approximated with a trapezoid. The area of each trapezoid is computed, and then all their areas are added together.\n",
    "\\begin{equation}\n",
    "\\int_a^b f(x)dx = \\frac{f(a) + f(b)}{2}\\Delta x  + \\sum_{i=0}^{n} [f(a + i\\Delta x) + f(a + (i+1)\\Delta x)]\\Delta x\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "# Arguments [a,b,n]\n",
    "a = 0       # Inf lim\n",
    "b = 50      # Sup lim\n",
    "n = 1000    # Number of trapezoids\n",
    "\n",
    "# Define the integrand\n",
    "def f(x):\n",
    "        return x**2 + x + 1\n",
    "\n",
    "# Trapezoidal rule\n",
    "def integrateRange(a, b, n):\n",
    "    integral = -(f(a) + f(b))/2.0\n",
    "    # n+1 endpoints, but n trapazoids\n",
    "    for x in np.linspace(a,b,n+1):\n",
    "        integral = integral + f(x)\n",
    "    integral = integral* (b-a)/n\n",
    "    return integral\n",
    "\n",
    "h = (b-a)/n # Step size.\n",
    "n_loc = int(n/size) # Trapezoids per process\n",
    "\n",
    "# Interval per process\n",
    "a_loc = a + rank*n_loc*h\n",
    "b_loc = a_loc + n_loc*h\n",
    "\n",
    "# Initializing variables.\n",
    "integral  = np.zeros(1) # Store the integral value here\n",
    "recv_buff = np.zeros(1) # Initial adress of recieiver buffer\n",
    "\n",
    "# Local tasks, each process integrates its own interval\n",
    "integral[0] = integrateRange(a_loc, b_loc, n_loc)\n",
    "\n",
    "# COMMUNICATION\n",
    "if rank == 0:\n",
    "    result = integral[0]\n",
    "    for i in range(1, size):\n",
    "        # rank 0 receives results from all processes\n",
    "        comm.Recv(recv_buff, ANY_SOURCE) \n",
    "        result += recv_buff[0]     # Sum all results\n",
    "else:\n",
    "    comm.Send(integral, dest=0) # Other processes send their result\n",
    "\n",
    "# root process prints results\n",
    "if rank == 0:\n",
    "    print('Compute the integral of f = x**2 + x +1 from',a,'to',b,\"and n =\", n, 'trapezoids')\n",
    "    #print(\"and n =\", n, 'trapezoids')\n",
    "    print('Estimated integral:   ', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Derivatives using global communication\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: justify\"> \n",
    "\n",
    "<p style=\"width:40%;float:right;padding-left:60px;padding-right:40px\">\n",
    "\\begin{equation}\n",
    "D_{ij} = \\frac{1}{dx}\n",
    " \\begin{pmatrix}\n",
    "  -1 &  1 &    &    & \\\\\n",
    "     & -1 &  1 &    & \\\\\n",
    "     &    & \\ddots  &  &  \\\\\n",
    "     &    &    & -1 &  1   \\\\\n",
    "     &    &    &    & -1\n",
    " \\end{pmatrix}\n",
    "\\end{equation}\n",
    "<span style=\"font-size:smaller\">\n",
    "</span>\n",
    "</p>\n",
    "\n",
    "Parallel implementation of numerical derivatives can be calculated via matrix-vecor product. we calculate derivatives by applying the differentiation matrix operator,$D_{ij}$, on the function one seeks to derivate. The next cell implements a finite difference upwind scheme, \n",
    "<br>\n",
    "<br>\n",
    "\\begin{equation}\n",
    "\\partial_x u(x,t) \\ = \\ \\lim_{dx \\to 0} \\frac{u(x+dx,t) - u(x,t)}{dx} \n",
    "\\end{equation}\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "def mat_vec(comm, A, x):\n",
    "    size = comm.Get_size()\n",
    "    n_proc = int(A.shape[0]/size)\n",
    "    if (A.shape[1] != x.size):\n",
    "        print('vector-matrix size mismatch')\n",
    "        comm.Abort()  \n",
    "    if(x.size % size != 0):\n",
    "        print('the number of processors must evenly divide n.')\n",
    "        comm.Abort()\n",
    "    # Initialize local matrices\n",
    "    A_proc = np.zeros((n_proc, A.shape[1]))  \n",
    "    # Divide and distribute matrice up\n",
    "    comm.Scatter( [A, MPI.DOUBLE], [A_proc, MPI.DOUBLE] )\n",
    "    # Broadcast vector x\n",
    "    comm.Bcast( [x, MPI.DOUBLE] )\n",
    "    #Return local computation of dot product\n",
    "    y_proc = A_proc @ x\n",
    "    return y_proc\n",
    "\n",
    "def FD_matrix(nx):\n",
    "    x, dx = np.linspace(-1, 1, nx+1, retstep=True)\n",
    "    D = np.zeros((nx+1, nx+1))\n",
    "    for i in range(1, nx+1):\n",
    "        for j in range(1, nx+1):\n",
    "            if i == j:\n",
    "                D[i, j] = 0\n",
    "            elif i == j + 1:\n",
    "                D[i, j] = -1\n",
    "            elif i + 1 == j:\n",
    "                D[i, j] = 1\n",
    "            else:\n",
    "                D[i, j] = 0\n",
    "    D = 0.5 * D / dx\n",
    "    return D, x\n",
    "\n",
    "def f(x):\n",
    "    s = .2 \n",
    "    f = np.exp(-1/s**2 * x**2)\n",
    "    return f\n",
    "def df_ana(x):\n",
    "    s = .2\n",
    "    df_ana = -2/s**2 * x * np.exp(-1/s**2 * x**2)\n",
    "    return df_ana\n",
    "      \n",
    "#---------------------------------------------\n",
    "#    PARALLEL IMPLENTATION\n",
    "#---------------------------------------------\n",
    "# Initialize space\n",
    "nx = 199     # Number of grid points\n",
    " \n",
    "# Initialize differentiation matrix\n",
    "D_fdiff, x = FD_matrix(nx)\n",
    "\n",
    "# Initialize vector to store result\n",
    "df = np.zeros(nx+1)\n",
    "\n",
    "# Parallel matrix-vector product, set you favorite Toeplitz matriz\n",
    "df_loc = mat_vec(comm, D_fdiff, f(x))\n",
    "# Gather local vectors  \n",
    "comm.Gather( [df_loc, MPI.DOUBLE], [df, MPI.DOUBLE], root=0 )\n",
    "\n",
    "# Plot derivatives    \n",
    "if (rank == 0):\n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(x, f(x), \"g\", lw = 1.5, label='Gaussian')\n",
    "    plt.legend(loc='upper right', shadow=True)\n",
    "    plt.xlabel('$x$')        \n",
    "    plt.ylabel('$f(x)$')\n",
    "    \n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(x, df_ana(x), \"b\", lw = 1.5, label='Analytical')\n",
    "    plt.plot(x, df, 'k--', lw = 1.5, label='Numerical')\n",
    "    plt.legend(loc='upper right', shadow=True)\n",
    "    plt.xlabel('$x$')        \n",
    "    plt.ylabel('$\\partial_x f(x)$')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 8. Derivatives using Point-To-Point communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
